{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "150949a3",
   "metadata": {},
   "source": [
    "# IVI Data Exploration and Cleaning Pipeline\n",
    "\n",
    "This notebook covers:\n",
    "1. Loading the raw SAS datasets\n",
    "2. Initial exploration (shape, dtypes, samples)\n",
    "3. Missing value analysis\n",
    "4. Outlier detection\n",
    "5. Data aggregation at contract level\n",
    "6. Creating a cleaned, unified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c632932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = Path('../data/KAU-Bupa')\n",
    "OUTPUT_DIR = Path('../data/processed')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Libraries loaded successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740d126",
   "metadata": {},
   "source": [
    "## 1. Load Raw Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3304d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Members loaded: (4263550, 9)\n"
     ]
    }
   ],
   "source": [
    "# Load SAS datasets\n",
    "print('Loading datasets...')\n",
    "\n",
    "# Load members data\n",
    "df_member = pd.read_sas(DATA_DIR / 'sampled_member.sas7bdat', encoding='latin1')\n",
    "print(f'Members loaded: {df_member.shape}')\n",
    "\n",
    "# Load claims data\n",
    "df_claims = pd.read_sas(DATA_DIR / 'sampled_claims.sas7bdat', encoding='latin1')\n",
    "print(f'Claims loaded: {df_claims.shape}')\n",
    "\n",
    "# Load calls data\n",
    "df_calls = pd.read_sas(DATA_DIR / 'sampled_calls.sas7bdat', encoding='latin1')\n",
    "print(f'Calls loaded: {df_calls.shape}')\n",
    "\n",
    "# Load preauth data\n",
    "df_preauth = pd.read_sas(DATA_DIR / 'sampled_preauth.sas7bdat', encoding='latin1')\n",
    "print(f'Preauth loaded: {df_preauth.shape}')\n",
    "\n",
    "print('\\nAll datasets loaded successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636da362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load provider info (Excel)\n",
    "df_provider = pd.read_excel(DATA_DIR / 'Provider_Info.xlsx')\n",
    "print(f'Provider Info loaded: {df_provider.shape}')\n",
    "df_provider.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7f61c",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eff353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display dataset info\n",
    "def explore_dataset(df, name):\n",
    "    print('=' * 60)\n",
    "    print(f'Dataset: {name}')\n",
    "    print('=' * 60)\n",
    "    print(f'Shape: {df.shape[0]:,} rows x {df.shape[1]} columns')\n",
    "    print(f'Memory usage: {df.memory_usage(deep=True).sum() / 1e6:.2f} MB')\n",
    "    print('\\nColumn Types:')\n",
    "    print(df.dtypes.value_counts())\n",
    "    print('\\nColumns:')\n",
    "    print(df.columns.tolist())\n",
    "    print('\\nFirst 3 rows:')\n",
    "    display(df.head(3))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2766208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Members dataset\n",
    "explore_dataset(df_member, 'Members')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87897fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Claims dataset\n",
    "explore_dataset(df_claims, 'Claims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd28895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Calls dataset\n",
    "explore_dataset(df_calls, 'Calls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6cbf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Preauth dataset\n",
    "explore_dataset(df_preauth, 'Preauth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3006c7c",
   "metadata": {},
   "source": [
    "## 3. Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b726df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_analysis(df, name):\n",
    "    \"\"\"Analyze missing values in a dataframe.\"\"\"\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': missing.index,\n",
    "        'Missing Count': missing.values,\n",
    "        'Missing %': missing_pct.values\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    print(f'\\n{name} - Missing Value Summary')\n",
    "    print('=' * 50)\n",
    "    if len(missing_df) == 0:\n",
    "        print('No missing values found!')\n",
    "    else:\n",
    "        print(f'Columns with missing values: {len(missing_df)}')\n",
    "        display(missing_df)\n",
    "    \n",
    "    return missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc35351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values for all datasets\n",
    "missing_member = missing_value_analysis(df_member, 'Members')\n",
    "missing_claims = missing_value_analysis(df_claims, 'Claims')\n",
    "missing_calls = missing_value_analysis(df_calls, 'Calls')\n",
    "missing_preauth = missing_value_analysis(df_preauth, 'Preauth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "datasets = [\n",
    "    (df_member, 'Members'),\n",
    "    (df_claims, 'Claims'),\n",
    "    (df_calls, 'Calls'),\n",
    "    (df_preauth, 'Preauth')\n",
    "]\n",
    "\n",
    "for ax, (df, name) in zip(axes.flatten(), datasets):\n",
    "    missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "    missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=True)\n",
    "    \n",
    "    if len(missing_pct) > 0:\n",
    "        missing_pct.plot(kind='barh', ax=ax, color='coral')\n",
    "        ax.set_xlabel('Missing %')\n",
    "        ax.set_title(f'{name} - Missing Values')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No missing values', ha='center', va='center', fontsize=12)\n",
    "        ax.set_title(f'{name} - Missing Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'missing_values_overview.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fecf5",
   "metadata": {},
   "source": [
    "## 4. Data Type Standardization and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab739ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names (uppercase, strip whitespace)\n",
    "def standardize_columns(df):\n",
    "    df.columns = df.columns.str.strip().str.upper()\n",
    "    return df\n",
    "\n",
    "df_member = standardize_columns(df_member)\n",
    "df_claims = standardize_columns(df_claims)\n",
    "df_calls = standardize_columns(df_calls)\n",
    "df_preauth = standardize_columns(df_preauth)\n",
    "df_provider = standardize_columns(df_provider)\n",
    "\n",
    "print('Column names standardized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c09eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns after standardization\n",
    "print('Members columns:', df_member.columns.tolist())\n",
    "print('\\nClaims columns:', df_claims.columns.tolist())\n",
    "print('\\nCalls columns:', df_calls.columns.tolist())\n",
    "print('\\nPreauth columns:', df_preauth.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode bytes columns if any exist\n",
    "def decode_bytes_columns(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            # Check if values are bytes\n",
    "            sample = df[col].dropna().head(1)\n",
    "            if len(sample) > 0 and isinstance(sample.iloc[0], bytes):\n",
    "                df[col] = df[col].apply(lambda x: x.decode('latin1') if isinstance(x, bytes) else x)\n",
    "                print(f'Decoded column: {col}')\n",
    "    return df\n",
    "\n",
    "df_member = decode_bytes_columns(df_member)\n",
    "df_claims = decode_bytes_columns(df_claims)\n",
    "df_calls = decode_bytes_columns(df_calls)\n",
    "df_preauth = decode_bytes_columns(df_preauth)\n",
    "\n",
    "print('\\nByte decoding complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0ca71",
   "metadata": {},
   "source": [
    "## 5. Key Statistics and Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ffb11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key statistics for Members\n",
    "print('MEMBERS - Key Statistics')\n",
    "print('=' * 50)\n",
    "print(f\"Unique Members (ADHERENT_NO): {df_member['ADHERENT_NO'].nunique():,}\")\n",
    "print(f\"Unique Contracts (CONTRACT_NO): {df_member['CONTRACT_NO'].nunique():,}\")\n",
    "print(f\"Unique Plans (PLAN_ID): {df_member['PLAN_ID'].nunique():,}\")\n",
    "print(f\"Contract periods (CONT_YYMM): {df_member['CONT_YYMM'].nunique()}\")\n",
    "print(f\"Date range: {df_member['CONT_YYMM'].min()} to {df_member['CONT_YYMM'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14978cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key statistics for Claims\n",
    "print('CLAIMS - Key Statistics')\n",
    "print('=' * 50)\n",
    "print(f\"Total claims rows: {len(df_claims):,}\")\n",
    "print(f\"Unique Members: {df_claims['ADHERENT_NO'].nunique():,}\")\n",
    "print(f\"Unique Contracts: {df_claims['CONT_NO'].nunique():,}\")\n",
    "print(f\"Unique Providers: {df_claims['PROV_CODE'].nunique():,}\")\n",
    "print(f\"Total Net Billed: {df_claims['SUM_OF_NETBILLED'].sum():,.2f}\")\n",
    "print(f\"Claim Types: {df_claims['CLAIM_TYPE'].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657a7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key statistics for Calls\n",
    "print('CALLS - Key Statistics')\n",
    "print('=' * 50)\n",
    "print(f\"Total calls: {len(df_calls):,}\")\n",
    "print(f\"Unique Contracts: {df_calls['CONT_NO'].nunique():,}\")\n",
    "print(f\"Call Categories: {df_calls['CALL_CAT'].nunique()}\")\n",
    "print(f\"\\nCall Category Distribution:\")\n",
    "print(df_calls['CALL_CAT'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcadb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key statistics for Preauth\n",
    "print('PREAUTH - Key Statistics')\n",
    "print('=' * 50)\n",
    "print(f\"Total preauth rows: {len(df_preauth):,}\")\n",
    "print(f\"Unique Contracts: {df_preauth['CONT_NO'].nunique():,}\")\n",
    "print(f\"Unique Episodes: {df_preauth['PREAUTH_EPISODE_ID'].nunique():,}\")\n",
    "print(f\"Episode Status Distribution:\")\n",
    "print(df_preauth['EPISODE_STATUS'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af51580",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01995fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(series, name):\n",
    "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "    \n",
    "    print(f'\\n{name} Outlier Analysis (IQR Method)')\n",
    "    print('-' * 40)\n",
    "    print(f'Q1: {Q1:,.2f}')\n",
    "    print(f'Q3: {Q3:,.2f}')\n",
    "    print(f'IQR: {IQR:,.2f}')\n",
    "    print(f'Lower bound: {lower_bound:,.2f}')\n",
    "    print(f'Upper bound: {upper_bound:,.2f}')\n",
    "    print(f'Outliers count: {len(outliers):,} ({len(outliers)/len(series)*100:.2f}%)')\n",
    "    \n",
    "    return outliers, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef793cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze outliers in Net Billed Amount (Claims)\n",
    "outliers_netbilled, lb_nb, ub_nb = detect_outliers_iqr(\n",
    "    df_claims['SUM_OF_NETBILLED'].dropna(), \n",
    "    'Net Billed Amount'\n",
    ")\n",
    "\n",
    "# Distribution plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram (capped for visibility)\n",
    "ax1 = axes[0]\n",
    "data_capped = df_claims['SUM_OF_NETBILLED'].clip(upper=df_claims['SUM_OF_NETBILLED'].quantile(0.99))\n",
    "ax1.hist(data_capped, bins=50, color='steelblue', edgecolor='white', alpha=0.7)\n",
    "ax1.axvline(ub_nb, color='red', linestyle='--', label=f'Upper bound: {ub_nb:,.0f}')\n",
    "ax1.set_xlabel('Net Billed Amount')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Net Billed Amount (99th percentile cap)')\n",
    "ax1.legend()\n",
    "\n",
    "# Box plot\n",
    "ax2 = axes[1]\n",
    "ax2.boxplot(data_capped, vert=True)\n",
    "ax2.set_ylabel('Net Billed Amount')\n",
    "ax2.set_title('Box Plot of Net Billed Amount')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'outliers_netbilled.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83981f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze outliers in Estimated Amount (Preauth)\n",
    "if 'EST_AMT' in df_preauth.columns:\n",
    "    outliers_estamt, lb_ea, ub_ea = detect_outliers_iqr(\n",
    "        df_preauth['EST_AMT'].dropna(), \n",
    "        'Estimated Amount (Preauth)'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb9a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Written Premium outliers (Members)\n",
    "if 'WP' in df_member.columns:\n",
    "    outliers_wp, lb_wp, ub_wp = detect_outliers_iqr(\n",
    "        df_member['WP'].dropna(), \n",
    "        'Written Premium'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4257755b",
   "metadata": {},
   "source": [
    "## 7. Data Aggregation at Contract Level\n",
    "\n",
    "We will aggregate data at the **Contract (CONT_NO)** level since IVI is evaluated per corporate client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Members data by Contract\n",
    "print('Aggregating Members data...')\n",
    "\n",
    "agg_member = df_member.groupby('CONTRACT_NO').agg(\n",
    "    total_members=('ADHERENT_NO', 'nunique'),\n",
    "    total_plans=('PLAN_ID', 'nunique'),\n",
    "    total_written_premium=('WP', 'sum'),\n",
    "    total_earned=('WE', 'sum'),\n",
    "    unique_networks=('PLAN_NETWORK', 'nunique'),\n",
    "    unique_nationalities=('NATIONALITY', 'nunique'),\n",
    "    male_count=('GENDER', lambda x: (x == 'M').sum() if x.dtype == 'object' else 0),\n",
    "    female_count=('GENDER', lambda x: (x == 'F').sum() if x.dtype == 'object' else 0),\n",
    ").reset_index()\n",
    "\n",
    "agg_member.columns = ['CONTRACT_NO', 'TOTAL_MEMBERS', 'TOTAL_PLANS', 'TOTAL_WRITTEN_PREMIUM', \n",
    "                      'TOTAL_EARNED', 'UNIQUE_NETWORKS', 'UNIQUE_NATIONALITIES', \n",
    "                      'MALE_COUNT', 'FEMALE_COUNT']\n",
    "\n",
    "print(f'Aggregated Members shape: {agg_member.shape}')\n",
    "agg_member.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Claims data by Contract\n",
    "print('Aggregating Claims data...')\n",
    "\n",
    "agg_claims = df_claims.groupby('CONT_NO').agg(\n",
    "    total_claims=('VOU_NO', 'count'),\n",
    "    unique_claims=('VOU_NO', 'nunique'),\n",
    "    total_net_billed=('SUM_OF_NETBILLED', 'sum'),\n",
    "    avg_net_billed=('SUM_OF_NETBILLED', 'mean'),\n",
    "    max_net_billed=('SUM_OF_NETBILLED', 'max'),\n",
    "    unique_members_with_claims=('ADHERENT_NO', 'nunique'),\n",
    "    unique_providers=('PROV_CODE', 'nunique'),\n",
    "    unique_diagnoses=('DIAG_CODE', 'nunique'),\n",
    "    unique_benefit_heads=('BEN_HEAD', 'nunique'),\n",
    ").reset_index()\n",
    "\n",
    "agg_claims.columns = ['CONTRACT_NO', 'TOTAL_CLAIM_LINES', 'UNIQUE_CLAIMS', 'TOTAL_NET_BILLED',\n",
    "                      'AVG_NET_BILLED', 'MAX_NET_BILLED', 'MEMBERS_WITH_CLAIMS',\n",
    "                      'UNIQUE_PROVIDERS', 'UNIQUE_DIAGNOSES', 'UNIQUE_BENEFIT_HEADS']\n",
    "\n",
    "print(f'Aggregated Claims shape: {agg_claims.shape}')\n",
    "agg_claims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Calls data by Contract\n",
    "print('Aggregating Calls data...')\n",
    "\n",
    "agg_calls = df_calls.groupby('CONT_NO').agg(\n",
    "    total_calls=('CALL_ID', 'count'),\n",
    "    unique_calls=('CALL_ID', 'nunique'),\n",
    "    unique_call_categories=('CALL_CAT', 'nunique'),\n",
    "    unique_callers=('MBR_NO', 'nunique'),\n",
    ").reset_index()\n",
    "\n",
    "agg_calls.columns = ['CONTRACT_NO', 'TOTAL_CALLS', 'UNIQUE_CALLS', \n",
    "                     'UNIQUE_CALL_CATEGORIES', 'UNIQUE_CALLERS']\n",
    "\n",
    "print(f'Aggregated Calls shape: {agg_calls.shape}')\n",
    "agg_calls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f27652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Preauth data by Contract\n",
    "print('Aggregating Preauth data...')\n",
    "\n",
    "agg_preauth = df_preauth.groupby('CONT_NO').agg(\n",
    "    total_preauth_items=('PREAUTH_EPISODE_ITEM_ID', 'count'),\n",
    "    unique_episodes=('PREAUTH_EPISODE_ID', 'nunique'),\n",
    "    unique_members_preauth=('MBR_NO', 'nunique'),\n",
    "    total_estimated_amt=('EST_AMT', 'sum'),\n",
    "    avg_estimated_amt=('EST_AMT', 'mean'),\n",
    "    unique_providers_preauth=('PROV_CODE', 'nunique'),\n",
    ").reset_index()\n",
    "\n",
    "agg_preauth.columns = ['CONTRACT_NO', 'TOTAL_PREAUTH_ITEMS', 'UNIQUE_PREAUTH_EPISODES',\n",
    "                       'MEMBERS_WITH_PREAUTH', 'TOTAL_ESTIMATED_AMT', 'AVG_ESTIMATED_AMT',\n",
    "                       'UNIQUE_PROVIDERS_PREAUTH']\n",
    "\n",
    "print(f'Aggregated Preauth shape: {agg_preauth.shape}')\n",
    "agg_preauth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2912d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate preauth approval/rejection rates\n",
    "print('Calculating preauth status rates...')\n",
    "\n",
    "preauth_status = df_preauth.groupby(['CONT_NO', 'EPISODE_STATUS']).size().unstack(fill_value=0)\n",
    "preauth_status['TOTAL'] = preauth_status.sum(axis=1)\n",
    "\n",
    "# Calculate rates for common statuses\n",
    "for col in preauth_status.columns:\n",
    "    if col != 'TOTAL':\n",
    "        preauth_status[f'{col}_RATE'] = preauth_status[col] / preauth_status['TOTAL']\n",
    "\n",
    "preauth_status = preauth_status.reset_index()\n",
    "preauth_status.columns = [str(c).upper().replace(' ', '_') for c in preauth_status.columns]\n",
    "preauth_status = preauth_status.rename(columns={'CONT_NO': 'CONTRACT_NO'})\n",
    "\n",
    "print(f'Preauth status rates shape: {preauth_status.shape}')\n",
    "preauth_status.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ebf321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate call category distribution per contract\n",
    "print('Calculating call category distribution...')\n",
    "\n",
    "call_cats = df_calls.groupby(['CONT_NO', 'CALL_CAT']).size().unstack(fill_value=0)\n",
    "call_cats = call_cats.add_prefix('CALLS_')\n",
    "call_cats = call_cats.reset_index()\n",
    "call_cats.columns = [str(c).upper().replace(' ', '_') for c in call_cats.columns]\n",
    "call_cats = call_cats.rename(columns={'CONT_NO': 'CONTRACT_NO'})\n",
    "\n",
    "print(f'Call categories shape: {call_cats.shape}')\n",
    "call_cats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a91b9",
   "metadata": {},
   "source": [
    "## 8. Merge All Aggregated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a75e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all aggregated datasets\n",
    "print('Merging all aggregated datasets...')\n",
    "\n",
    "# Start with members (base)\n",
    "df_contract = agg_member.copy()\n",
    "print(f'After members: {df_contract.shape}')\n",
    "\n",
    "# Merge claims\n",
    "df_contract = df_contract.merge(agg_claims, on='CONTRACT_NO', how='left')\n",
    "print(f'After claims: {df_contract.shape}')\n",
    "\n",
    "# Merge calls\n",
    "df_contract = df_contract.merge(agg_calls, on='CONTRACT_NO', how='left')\n",
    "print(f'After calls: {df_contract.shape}')\n",
    "\n",
    "# Merge preauth\n",
    "df_contract = df_contract.merge(agg_preauth, on='CONTRACT_NO', how='left')\n",
    "print(f'After preauth: {df_contract.shape}')\n",
    "\n",
    "# Merge preauth status rates (select key columns)\n",
    "status_cols = ['CONTRACT_NO'] + [c for c in preauth_status.columns if '_RATE' in c]\n",
    "df_contract = df_contract.merge(preauth_status[status_cols], on='CONTRACT_NO', how='left')\n",
    "print(f'After preauth status: {df_contract.shape}')\n",
    "\n",
    "print(f'\\nFinal merged dataset shape: {df_contract.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962108e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the merged dataset\n",
    "df_contract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any remaining missing values after merge\n",
    "missing_after_merge = df_contract.isnull().sum()\n",
    "missing_after_merge = missing_after_merge[missing_after_merge > 0]\n",
    "\n",
    "print('Missing values after merge:')\n",
    "print(missing_after_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c42b190",
   "metadata": {},
   "source": [
    "## 9. Create Derived Features (IVI Components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create derived features for IVI calculation\n",
    "print('Creating derived features...')\n",
    "\n",
    "# Utilization metrics\n",
    "df_contract['UTILIZATION_RATE'] = df_contract['MEMBERS_WITH_CLAIMS'] / df_contract['TOTAL_MEMBERS']\n",
    "df_contract['CLAIMS_PER_MEMBER'] = df_contract['TOTAL_CLAIM_LINES'] / df_contract['TOTAL_MEMBERS']\n",
    "df_contract['COST_PER_MEMBER'] = df_contract['TOTAL_NET_BILLED'] / df_contract['TOTAL_MEMBERS']\n",
    "df_contract['COST_PER_UTILIZER'] = df_contract['TOTAL_NET_BILLED'] / df_contract['MEMBERS_WITH_CLAIMS'].replace(0, np.nan)\n",
    "\n",
    "# Loss ratio (key sustainability metric)\n",
    "df_contract['LOSS_RATIO'] = df_contract['TOTAL_NET_BILLED'] / df_contract['TOTAL_EARNED'].replace(0, np.nan)\n",
    "\n",
    "# Experience metrics\n",
    "df_contract['CALLS_PER_MEMBER'] = df_contract['TOTAL_CALLS'] / df_contract['TOTAL_MEMBERS']\n",
    "df_contract['PREAUTH_PER_MEMBER'] = df_contract['UNIQUE_PREAUTH_EPISODES'] / df_contract['TOTAL_MEMBERS']\n",
    "\n",
    "# Gender ratio\n",
    "df_contract['MALE_RATIO'] = df_contract['MALE_COUNT'] / df_contract['TOTAL_MEMBERS']\n",
    "\n",
    "print('Derived features created.')\n",
    "print(f'\\nNew columns: {[c for c in df_contract.columns if c not in agg_member.columns]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953781d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with appropriate defaults\n",
    "print('Handling missing values in derived features...')\n",
    "\n",
    "# For contracts with no claims/calls/preauth, fill with 0\n",
    "fill_zero_cols = ['TOTAL_CLAIM_LINES', 'UNIQUE_CLAIMS', 'TOTAL_NET_BILLED', 'AVG_NET_BILLED',\n",
    "                  'MAX_NET_BILLED', 'MEMBERS_WITH_CLAIMS', 'UNIQUE_PROVIDERS', 'UNIQUE_DIAGNOSES',\n",
    "                  'UNIQUE_BENEFIT_HEADS', 'TOTAL_CALLS', 'UNIQUE_CALLS', 'UNIQUE_CALL_CATEGORIES',\n",
    "                  'UNIQUE_CALLERS', 'TOTAL_PREAUTH_ITEMS', 'UNIQUE_PREAUTH_EPISODES',\n",
    "                  'MEMBERS_WITH_PREAUTH', 'TOTAL_ESTIMATED_AMT', 'AVG_ESTIMATED_AMT',\n",
    "                  'UNIQUE_PROVIDERS_PREAUTH', 'UTILIZATION_RATE', 'CLAIMS_PER_MEMBER',\n",
    "                  'COST_PER_MEMBER', 'CALLS_PER_MEMBER', 'PREAUTH_PER_MEMBER']\n",
    "\n",
    "for col in fill_zero_cols:\n",
    "    if col in df_contract.columns:\n",
    "        df_contract[col] = df_contract[col].fillna(0)\n",
    "\n",
    "# Fill rate columns with 0\n",
    "rate_cols = [c for c in df_contract.columns if '_RATE' in c]\n",
    "for col in rate_cols:\n",
    "    df_contract[col] = df_contract[col].fillna(0)\n",
    "\n",
    "# Check remaining missing\n",
    "remaining_missing = df_contract.isnull().sum()\n",
    "remaining_missing = remaining_missing[remaining_missing > 0]\n",
    "print(f'\\nRemaining missing values: {len(remaining_missing)}')\n",
    "if len(remaining_missing) > 0:\n",
    "    print(remaining_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549637d3",
   "metadata": {},
   "source": [
    "## 10. Final Dataset Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset summary\n",
    "print('FINAL DATASET SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'Shape: {df_contract.shape[0]:,} contracts x {df_contract.shape[1]} features')\n",
    "print(f'\\nMemory usage: {df_contract.memory_usage(deep=True).sum() / 1e6:.2f} MB')\n",
    "print(f'\\nColumn types:')\n",
    "print(df_contract.dtypes.value_counts())\n",
    "print(f'\\nMissing values: {df_contract.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e698ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "df_contract.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd50403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns in final dataset\n",
    "print('All columns in final dataset:')\n",
    "for i, col in enumerate(df_contract.columns, 1):\n",
    "    print(f'{i:2}. {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02527604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset\n",
    "output_file = OUTPUT_DIR / 'contract_aggregated_data.parquet'\n",
    "df_contract.to_parquet(output_file, index=False)\n",
    "print(f'Dataset saved to: {output_file}')\n",
    "\n",
    "# Also save as CSV for easy inspection\n",
    "csv_file = OUTPUT_DIR / 'contract_aggregated_data.csv'\n",
    "df_contract.to_csv(csv_file, index=False)\n",
    "print(f'CSV backup saved to: {csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Contract size distribution\n",
    "axes[0, 0].hist(df_contract['TOTAL_MEMBERS'].clip(upper=df_contract['TOTAL_MEMBERS'].quantile(0.99)), \n",
    "                bins=50, color='steelblue', edgecolor='white')\n",
    "axes[0, 0].set_xlabel('Total Members')\n",
    "axes[0, 0].set_title('Contract Size Distribution')\n",
    "\n",
    "# Loss ratio distribution\n",
    "axes[0, 1].hist(df_contract['LOSS_RATIO'].dropna().clip(upper=3), \n",
    "                bins=50, color='coral', edgecolor='white')\n",
    "axes[0, 1].axvline(1.0, color='red', linestyle='--', label='Break-even (100%)')\n",
    "axes[0, 1].set_xlabel('Loss Ratio')\n",
    "axes[0, 1].set_title('Loss Ratio Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Utilization rate\n",
    "axes[0, 2].hist(df_contract['UTILIZATION_RATE'].dropna(), \n",
    "                bins=50, color='green', edgecolor='white')\n",
    "axes[0, 2].set_xlabel('Utilization Rate')\n",
    "axes[0, 2].set_title('Member Utilization Rate')\n",
    "\n",
    "# Cost per member\n",
    "axes[1, 0].hist(df_contract['COST_PER_MEMBER'].clip(upper=df_contract['COST_PER_MEMBER'].quantile(0.99)), \n",
    "                bins=50, color='purple', edgecolor='white')\n",
    "axes[1, 0].set_xlabel('Cost per Member')\n",
    "axes[1, 0].set_title('Cost per Member Distribution')\n",
    "\n",
    "# Calls per member\n",
    "axes[1, 1].hist(df_contract['CALLS_PER_MEMBER'].clip(upper=df_contract['CALLS_PER_MEMBER'].quantile(0.99)), \n",
    "                bins=50, color='orange', edgecolor='white')\n",
    "axes[1, 1].set_xlabel('Calls per Member')\n",
    "axes[1, 1].set_title('Calls per Member Distribution')\n",
    "\n",
    "# Written premium distribution\n",
    "axes[1, 2].hist(df_contract['TOTAL_WRITTEN_PREMIUM'].clip(upper=df_contract['TOTAL_WRITTEN_PREMIUM'].quantile(0.99)), \n",
    "                bins=50, color='teal', edgecolor='white')\n",
    "axes[1, 2].set_xlabel('Total Written Premium')\n",
    "axes[1, 2].set_title('Written Premium Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'contract_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7653a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for key metrics\n",
    "key_metrics = ['TOTAL_MEMBERS', 'TOTAL_WRITTEN_PREMIUM', 'TOTAL_NET_BILLED', 'LOSS_RATIO',\n",
    "               'UTILIZATION_RATE', 'COST_PER_MEMBER', 'CLAIMS_PER_MEMBER', 'CALLS_PER_MEMBER',\n",
    "               'PREAUTH_PER_MEMBER']\n",
    "\n",
    "corr_matrix = df_contract[key_metrics].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, \n",
    "            fmt='.2f', square=True, linewidths=0.5)\n",
    "plt.title('Correlation Matrix - Key Contract Metrics')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02e867c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Data Processing Completed:\n",
    "1. Loaded 4 SAS datasets (members, claims, calls, preauth)\n",
    "2. Standardized column names and decoded byte strings\n",
    "3. Analyzed missing values across all datasets\n",
    "4. Detected outliers using IQR method\n",
    "5. Aggregated all data at contract level\n",
    "6. Created derived features for IVI calculation\n",
    "7. Saved cleaned dataset to parquet and CSV formats\n",
    "\n",
    "### Output Files:\n",
    "- `data/processed/contract_aggregated_data.parquet`\n",
    "- `data/processed/contract_aggregated_data.csv`\n",
    "- `data/processed/missing_values_overview.png`\n",
    "- `data/processed/outliers_netbilled.png`\n",
    "- `data/processed/contract_distributions.png`\n",
    "- `data/processed/correlation_matrix.png`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zoot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
